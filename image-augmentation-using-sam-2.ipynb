{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9241956,"sourceType":"datasetVersion","datasetId":5590552}],"dockerImageVersionId":30762,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Image Augmentation using META SAM-2 Model and Stability AIÂ¶\n","metadata":{}},{"cell_type":"markdown","source":"Importing Images with Annoted text file for Yolov8n Model Training","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"id":"SVQDDHrZQX9p","execution":{"iopub.status.busy":"2024-08-25T04:53:07.745420Z","iopub.execute_input":"2024-08-25T04:53:07.745749Z","iopub.status.idle":"2024-08-25T04:53:08.129767Z","shell.execute_reply.started":"2024-08-25T04:53:07.745710Z","shell.execute_reply":"2024-08-25T04:53:08.128882Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Image segmenting","metadata":{}},{"cell_type":"markdown","source":"importing SAM-2 model (may take a while to download)\n","metadata":{}},{"cell_type":"code","source":"!git clone https://github.com/facebookresearch/segment-anything-2.git\n%cd /kaggle/working/segment-anything-2\n%pip install -e .\n%cd /kaggle/working/segment-anything-2/checkpoints\n!bash /kaggle/working/segment-anything-2/checkpoints/download_ckpts.sh\n%cd /kaggle/working/segment-anything-2","metadata":{"id":"C5lnsWaDQ0ZC","outputId":"e3299e8c-dbf7-4e99-9509-ef28f2bdef59","execution":{"iopub.status.busy":"2024-08-25T04:53:08.131539Z","iopub.execute_input":"2024-08-25T04:53:08.131925Z","iopub.status.idle":"2024-08-25T04:57:22.783294Z","shell.execute_reply.started":"2024-08-25T04:53:08.131891Z","shell.execute_reply":"2024-08-25T04:57:22.782231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n","metadata":{"id":"74s-K70-Q9YB","execution":{"iopub.status.busy":"2024-08-25T04:57:22.784743Z","iopub.execute_input":"2024-08-25T04:57:22.785071Z","iopub.status.idle":"2024-08-25T04:57:26.462635Z","shell.execute_reply.started":"2024-08-25T04:57:22.785035Z","shell.execute_reply":"2024-08-25T04:57:26.461717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# use bfloat16 for the entire notebook\ntorch.autocast(device_type=\"cuda\", dtype=torch.bfloat16).__enter__()\n\nif torch.cuda.get_device_properties(0).major >= 8:\n    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True","metadata":{"id":"Uny2L-3sREaK","execution":{"iopub.status.busy":"2024-08-25T04:57:26.465331Z","iopub.execute_input":"2024-08-25T04:57:26.465868Z","iopub.status.idle":"2024-08-25T04:57:26.725195Z","shell.execute_reply.started":"2024-08-25T04:57:26.465812Z","shell.execute_reply":"2024-08-25T04:57:26.724410Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Displaying Image\n","metadata":{}},{"cell_type":"code","source":"image = Image.open('/kaggle/input/avengers/maxresdefault.jpg')\nimage = np.array(image.convert(\"RGB\"))\nplt.figure(figsize=(20,20))\nplt.imshow(image)\nplt.axis('off')\nplt.show()","metadata":{"id":"soYeuzszRPZb","outputId":"1ec478ef-19f0-43fc-95c5-5f91708e98c1","execution":{"iopub.status.busy":"2024-08-25T04:57:28.137876Z","iopub.execute_input":"2024-08-25T04:57:28.138201Z","iopub.status.idle":"2024-08-25T04:57:29.417760Z","shell.execute_reply.started":"2024-08-25T04:57:28.138166Z","shell.execute_reply":"2024-08-25T04:57:29.416369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nfrom sam2.build_sam import build_sam2\nfrom sam2.sam2_image_predictor import SAM2ImagePredictor","metadata":{"id":"d3zUJOY5RXU6","execution":{"iopub.status.busy":"2024-08-25T04:57:45.531192Z","iopub.execute_input":"2024-08-25T04:57:45.531600Z","iopub.status.idle":"2024-08-25T04:57:47.051318Z","shell.execute_reply.started":"2024-08-25T04:57:45.531561Z","shell.execute_reply":"2024-08-25T04:57:47.050513Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Functions to display mask and points on image","metadata":{}},{"cell_type":"code","source":"def show_mask(mask, ax, random_color=False, borders = True):\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n    h, w = mask.shape[-2:]\n    mask = mask.astype(np.uint8)\n    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n    if borders:\n        import cv2\n        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n        # Try to smooth contours\n        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2)\n    ax.imshow(mask_image)\n\ndef show_points(coords, labels, ax, marker_size=375):\n    pos_points = coords[labels==1]\n    neg_points = coords[labels==0]\n    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n\ndef show_box(box, ax):\n    x0, y0 = box[0], box[1]\n    w, h = box[2] - box[0], box[3] - box[1]\n    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))\n\ndef show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n    for i, (mask, score) in enumerate(zip(masks, scores)):\n        plt.figure(figsize=(10, 10))\n        plt.imshow(image)\n        show_mask(mask, plt.gca(), borders=borders)\n        if point_coords is not None:\n            assert input_labels is not None\n            show_points(point_coords, input_labels, plt.gca())\n        if box_coords is not None:\n            # boxes\n            show_box(box_coords, plt.gca())\n        if len(scores) > 1:\n            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n        plt.axis('off')\n        plt.show()","metadata":{"id":"2kWy0IZlRnbQ","execution":{"iopub.status.busy":"2024-08-25T04:57:51.026568Z","iopub.execute_input":"2024-08-25T04:57:51.027734Z","iopub.status.idle":"2024-08-25T04:57:51.043957Z","shell.execute_reply.started":"2024-08-25T04:57:51.027689Z","shell.execute_reply":"2024-08-25T04:57:51.043105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nplt.imshow(image)\nplt.axis('on')\nplt.show()","metadata":{"id":"DaerCHXBRnXJ","outputId":"88eda314-35aa-44ab-f629-835656439a76","execution":{"iopub.status.busy":"2024-08-25T04:57:56.145179Z","iopub.execute_input":"2024-08-25T04:57:56.146098Z","iopub.status.idle":"2024-08-25T04:57:56.689128Z","shell.execute_reply.started":"2024-08-25T04:57:56.146050Z","shell.execute_reply":"2024-08-25T04:57:56.688238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sam2_checkpoint = \"/kaggle/working/segment-anything-2/checkpoints/sam2_hiera_base_plus.pt\"\nmodel_cfg = \"sam2_hiera_b+.yaml\"\n\nsam2_model = build_sam2(model_cfg, sam2_checkpoint, device=\"cuda\")\n\npredictor = SAM2ImagePredictor(sam2_model)","metadata":{"id":"h2WNhHwjRnTr","execution":{"iopub.status.busy":"2024-08-25T05:00:46.858532Z","iopub.execute_input":"2024-08-25T05:00:46.859270Z","iopub.status.idle":"2024-08-25T05:00:48.400271Z","shell.execute_reply.started":"2024-08-25T05:00:46.859229Z","shell.execute_reply":"2024-08-25T05:00:48.399359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictor.set_image(image)","metadata":{"id":"gj5Ll5onRnQZ","execution":{"iopub.status.busy":"2024-08-25T05:00:54.312838Z","iopub.execute_input":"2024-08-25T05:00:54.313632Z","iopub.status.idle":"2024-08-25T05:00:55.233121Z","shell.execute_reply.started":"2024-08-25T05:00:54.313591Z","shell.execute_reply":"2024-08-25T05:00:55.232334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_point = np.array([[370,400]])\ninput_label = np.array([1])","metadata":{"id":"qEP8F3BrRnMm","execution":{"iopub.status.busy":"2024-08-25T05:01:27.728228Z","iopub.execute_input":"2024-08-25T05:01:27.728943Z","iopub.status.idle":"2024-08-25T05:01:27.733598Z","shell.execute_reply.started":"2024-08-25T05:01:27.728899Z","shell.execute_reply":"2024-08-25T05:01:27.732498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nplt.imshow(image)\nshow_points(input_point, input_label, plt.gca())\nplt.axis('on')\nplt.show()","metadata":{"id":"ynOQYKBvRnJO","outputId":"07e2b9f9-3d98-4d35-823f-0cd704fc56d4","execution":{"iopub.status.busy":"2024-08-25T05:01:28.245390Z","iopub.execute_input":"2024-08-25T05:01:28.246408Z","iopub.status.idle":"2024-08-25T05:01:28.842016Z","shell.execute_reply.started":"2024-08-25T05:01:28.246368Z","shell.execute_reply":"2024-08-25T05:01:28.841131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(predictor._features[\"image_embed\"].shape, predictor._features[\"image_embed\"][-1].shape)","metadata":{"id":"i8HUn4f0RnGB","outputId":"ee8e7cdb-ed43-42fc-e199-04cb844aaa88","execution":{"iopub.status.busy":"2024-08-25T05:01:37.341983Z","iopub.execute_input":"2024-08-25T05:01:37.342871Z","iopub.status.idle":"2024-08-25T05:01:37.348130Z","shell.execute_reply.started":"2024-08-25T05:01:37.342829Z","shell.execute_reply":"2024-08-25T05:01:37.347243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks, scores, logits = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    multimask_output=True,\n)\nsorted_ind = np.argsort(scores)[::-1]\nmasks = masks[sorted_ind]\nscores = scores[sorted_ind]\nlogits = logits[sorted_ind]","metadata":{"id":"3Jv6ufO6RnC3","execution":{"iopub.status.busy":"2024-08-25T05:01:38.727353Z","iopub.execute_input":"2024-08-25T05:01:38.727750Z","iopub.status.idle":"2024-08-25T05:01:39.051857Z","shell.execute_reply.started":"2024-08-25T05:01:38.727713Z","shell.execute_reply":"2024-08-25T05:01:39.050688Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label, borders=True)","metadata":{"id":"6lStCWZNRm-F","outputId":"3026cf16-995b-4c27-a923-576741e12596","execution":{"iopub.status.busy":"2024-08-25T05:01:48.885262Z","iopub.execute_input":"2024-08-25T05:01:48.886017Z","iopub.status.idle":"2024-08-25T05:01:51.221972Z","shell.execute_reply.started":"2024-08-25T05:01:48.885977Z","shell.execute_reply":"2024-08-25T05:01:51.220986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef show_masked_area(image, mask, ax, random_color=False):\n    # Set the color of the mask (optional)\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n\n    # Ensure the mask is in the correct shape\n    h, w = mask.shape[-2:]\n    mask = mask.astype(np.uint8)\n\n    # Apply mask to the image\n    masked_image = np.zeros_like(image)\n    for c in range(3):  # Assuming the image has 3 color channels\n        masked_image[..., c] = image[..., c] * mask\n\n    # Show only the masked area\n    ax.imshow(masked_image)\n\ndef show_masks_1(image, masks, scores):\n    for i, (mask, score) in enumerate(zip(masks, scores)):\n        fig, ax = plt.subplots(figsize=(10, 10))\n        show_masked_area(image, mask, ax)\n        plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n        plt.axis('off')\n        plt.show()\n\nshow_masks_1(image, masks, scores)\n","metadata":{"id":"uri_N9lvRm7Q","outputId":"c78d1a0e-bab6-48cb-b46b-130f6ec1ee30","execution":{"iopub.status.busy":"2024-08-25T05:02:32.774307Z","iopub.execute_input":"2024-08-25T05:02:32.775256Z","iopub.status.idle":"2024-08-25T05:02:33.719522Z","shell.execute_reply.started":"2024-08-25T05:02:32.775213Z","shell.execute_reply":"2024-08-25T05:02:33.718626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 10))\nplt.imshow(image)\nplt.axis('on')\nplt.show()","metadata":{"id":"AbxduNFaRm4O","outputId":"37a32007-ee91-4dc6-d0b6-4d4ffcf23b38","execution":{"iopub.status.busy":"2024-08-25T05:02:33.721390Z","iopub.execute_input":"2024-08-25T05:02:33.721806Z","iopub.status.idle":"2024-08-25T05:02:34.334026Z","shell.execute_reply.started":"2024-08-25T05:02:33.721760Z","shell.execute_reply":"2024-08-25T05:02:34.333115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_point = np.array([[370, 400],[400,350]])\ninput_label = np.array([1,1])\n\nmask_input = logits[np.argmax(scores), :, :]","metadata":{"id":"dhXr1K4gRm1Y","execution":{"iopub.status.busy":"2024-08-25T05:04:35.987903Z","iopub.execute_input":"2024-08-25T05:04:35.988287Z","iopub.status.idle":"2024-08-25T05:04:35.993456Z","shell.execute_reply.started":"2024-08-25T05:04:35.988254Z","shell.execute_reply":"2024-08-25T05:04:35.992499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masks, scores, _ = predictor.predict(\n    point_coords=input_point,\n    point_labels=input_label,\n    mask_input=mask_input[None, :, :],\n    multimask_output=False,\n)","metadata":{"id":"Fzo8SUl-RmyW","execution":{"iopub.status.busy":"2024-08-25T05:04:37.502964Z","iopub.execute_input":"2024-08-25T05:04:37.503320Z","iopub.status.idle":"2024-08-25T05:04:37.529261Z","shell.execute_reply.started":"2024-08-25T05:04:37.503289Z","shell.execute_reply":"2024-08-25T05:04:37.528509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show_masks(image, masks, scores, point_coords=input_point, input_labels=input_label)","metadata":{"id":"MtfygOHcRmu0","outputId":"87ed78dd-418a-42d3-a503-f52cd62e6ad2","execution":{"iopub.status.busy":"2024-08-25T05:04:37.734295Z","iopub.execute_input":"2024-08-25T05:04:37.734696Z","iopub.status.idle":"2024-08-25T05:04:38.390728Z","shell.execute_reply.started":"2024-08-25T05:04:37.734659Z","shell.execute_reply":"2024-08-25T05:04:38.389679Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef show_masked_area(image, mask, ax, random_color=False):\n    # Set the color of the mask (optional)\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n\n    # Ensure the mask is in the correct shape\n    h, w = mask.shape[-2:]\n    mask = mask.astype(np.uint8)\n\n    # Apply mask to the image\n    masked_image = np.zeros_like(image)\n    for c in range(3):  # Assuming the image has 3 color channels\n        masked_image[..., c] = image[..., c] * mask\n\n    # Show only the masked area\n    ax.imshow(masked_image)\n\ndef show_masks_1(image, masks, scores):\n    for i, (mask, score) in enumerate(zip(masks, scores)):\n        fig, ax = plt.subplots(figsize=(10, 10))\n        show_masked_area(image, mask, ax)\n        plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n        plt.axis('off')\n        plt.show()\n\nshow_masks_1(image, masks, scores)","metadata":{"id":"mz9-X7YuSKj3","outputId":"80d287ff-f166-40b4-88a0-0ba8322b1e65","execution":{"iopub.status.busy":"2024-08-25T05:04:43.708066Z","iopub.execute_input":"2024-08-25T05:04:43.708831Z","iopub.status.idle":"2024-08-25T05:04:44.034361Z","shell.execute_reply.started":"2024-08-25T05:04:43.708774Z","shell.execute_reply":"2024-08-25T05:04:44.033448Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef show_masked_area(image, mask, ax, random_color=False):\n    \"\"\"\n    Display a masked area of an image on a given matplotlib axis.\n\n    Parameters:\n    - image: NumPy array of shape (height, width, 3), representing the image.\n    - mask: Binary NumPy array of shape (height, width), representing the mask.\n    - ax: Matplotlib axis object where the mask will be displayed.\n    - random_color: Boolean, whether to use a random color for the mask.\n    \"\"\"\n    # Set the color of the mask (optional)\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n\n    # Ensure the mask is in the correct shape\n    h, w = mask.shape[-2:]\n    mask = mask.astype(np.uint8)\n\n    # Apply mask to the image\n    masked_image = np.zeros_like(image)\n    for c in range(3):  # Assuming the image has 3 color channels\n        masked_image[..., c] = image[..., c] * mask\n\n    # Show only the masked area\n    ax.imshow(masked_image)\n\ndef save_masked_images(image, masks, scores, save_dir=\"/kaggle/working/sam_2_augmentation/masked_images\", random_color=False):\n    \"\"\"\n    Save masked images to a specified directory.\n\n    Parameters:\n    - image: NumPy array of shape (height, width, 3), representing the image.\n    - masks: List of binary NumPy arrays, each of shape (height, width), representing the masks.\n    - scores: List of scores associated with each mask.\n    - save_dir: String, the directory to save the masked images.\n    - random_color: Boolean, whether to use a random color for each mask.\n    \"\"\"\n    # Create the directory if it doesn't exist\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    for i, (mask, score) in enumerate(zip(masks, scores)):\n        fig, ax = plt.subplots(figsize=(10, 10))\n        show_masked_area(image, mask, ax, random_color=random_color)\n        plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n        plt.axis('off')\n\n        # Save the figure\n        filename = f\"{save_dir}/masked_image_{i+1}.png\"\n        plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n        print(f\"Saved {filename}\")\n\n        plt.close(fig)  # Close the figure to free up memory\n","metadata":{"id":"VlrXixPZSKhM","execution":{"iopub.status.busy":"2024-08-25T05:07:11.077011Z","iopub.execute_input":"2024-08-25T05:07:11.077725Z","iopub.status.idle":"2024-08-25T05:07:11.089495Z","shell.execute_reply.started":"2024-08-25T05:07:11.077685Z","shell.execute_reply":"2024-08-25T05:07:11.088361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\nif __name__ == \"__main__\":\n    save_masked_images(image, masks, scores)\n","metadata":{"id":"WVdoPOd9SKex","outputId":"9952b55d-ce48-4b0d-9e08-2bbe7d58a043","execution":{"iopub.status.busy":"2024-08-25T05:07:17.258155Z","iopub.execute_input":"2024-08-25T05:07:17.258571Z","iopub.status.idle":"2024-08-25T05:07:17.664130Z","shell.execute_reply.started":"2024-08-25T05:07:17.258533Z","shell.execute_reply":"2024-08-25T05:07:17.663123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mask_img = Image.open('/kaggle/working/sam_2_augmentation/masked_images/masked_image_1.png')\nmask_img = np.array(mask_img.convert(\"RGB\"))\nplt.figure(figsize=(20,20))\nplt.imshow(mask_img)\nplt.axis('off')\nplt.show()","metadata":{"id":"3XMGiK1pSKcV","outputId":"f11d4ed4-2e88-4e51-8700-880855109050","execution":{"iopub.status.busy":"2024-08-25T05:07:53.119674Z","iopub.execute_input":"2024-08-25T05:07:53.120446Z","iopub.status.idle":"2024-08-25T05:07:53.416817Z","shell.execute_reply.started":"2024-08-25T05:07:53.120407Z","shell.execute_reply":"2024-08-25T05:07:53.415851Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\ndef show_inverse_masked_area(image, mask, ax, random_color=False):\n    \"\"\"\n    Display an inverse masked area of an image on a given matplotlib axis.\n\n    Parameters:\n    - image: NumPy array of shape (height, width, 3), representing the image.\n    - mask: Binary NumPy array of shape (height, width), representing the mask.\n    - ax: Matplotlib axis object where the inverse mask will be displayed.\n    - random_color: Boolean, whether to use a random color for the mask.\n    \"\"\"\n    # Set the color of the mask (optional)\n    if random_color:\n        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n    else:\n        color = np.array([30/255, 144/255, 255/255, 0.6])\n\n    # Ensure the mask is in the correct shape\n    h, w = mask.shape[-2:]\n    mask = mask.astype(np.uint8)\n\n    # Invert the mask\n    inverse_mask = 1 - mask\n\n    # Apply inverse mask to the image\n    inverse_masked_image = np.zeros_like(image)\n    for c in range(3):  # Assuming the image has 3 color channels\n        inverse_masked_image[..., c] = image[..., c] * inverse_mask\n\n    # Show only the inverse masked area\n    ax.imshow(inverse_masked_image)\n\ndef save_inverse_masked_images(image, masks, scores, save_dir=\"/kaggle/working/sam_2_augmentation/inverse_masked_images\", random_color=False):\n    \"\"\"\n    Save inverse masked images to a specified directory.\n\n    Parameters:\n    - image: NumPy array of shape (height, width, 3), representing the image.\n    - masks: List of binary NumPy arrays, each of shape (height, width), representing the masks.\n    - scores: List of scores associated with each mask.\n    - save_dir: String, the directory to save the inverse masked images.\n    - random_color: Boolean, whether to use a random color for each mask.\n    \"\"\"\n    # Create the directory if it doesn't exist\n    if not os.path.exists(save_dir):\n        os.makedirs(save_dir)\n\n    for i, (mask, score) in enumerate(zip(masks, scores)):\n        fig, ax = plt.subplots(figsize=(10, 10))\n        show_inverse_masked_area(image, mask, ax, random_color=random_color)\n        plt.title(f\"Inverse Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n        plt.axis('off')\n\n        # Save the figure\n        filename = f\"{save_dir}/inverse_masked_image_{i+1}.png\"\n        plt.savefig(filename, bbox_inches='tight', pad_inches=0)\n        print(f\"Saved {filename}\")\n\n        plt.close(fig)  # Close the figure to free up memory\n","metadata":{"id":"qqgPnJgMSKZ1","execution":{"iopub.status.busy":"2024-08-25T05:08:00.473304Z","iopub.execute_input":"2024-08-25T05:08:00.474130Z","iopub.status.idle":"2024-08-25T05:08:00.486048Z","shell.execute_reply.started":"2024-08-25T05:08:00.474089Z","shell.execute_reply":"2024-08-25T05:08:00.485045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\nif __name__ == \"__main__\":\n    save_inverse_masked_images(image, masks, scores)\n","metadata":{"id":"fAc_mNm0S3Ip","outputId":"91db97b0-52b4-4836-9c58-16e6173d1117","execution":{"iopub.status.busy":"2024-08-25T05:08:01.515044Z","iopub.execute_input":"2024-08-25T05:08:01.515434Z","iopub.status.idle":"2024-08-25T05:08:02.119730Z","shell.execute_reply.started":"2024-08-25T05:08:01.515398Z","shell.execute_reply":"2024-08-25T05:08:02.118491Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inv_mask_img = Image.open('/kaggle/working/sam_2_augmentation/inverse_masked_images/inverse_masked_image_1.png')\ninv_mask_img = np.array(inv_mask_img.convert(\"RGB\"))\nplt.figure(figsize=(20,20))\nplt.imshow(inv_mask_img)\nplt.axis('off')\nplt.show()","metadata":{"id":"IBn4ckPvS3Ff","outputId":"135ecb84-5e5c-4de9-b21c-0098e358572b","execution":{"iopub.status.busy":"2024-08-25T05:08:02.347000Z","iopub.execute_input":"2024-08-25T05:08:02.347625Z","iopub.status.idle":"2024-08-25T05:08:02.834101Z","shell.execute_reply.started":"2024-08-25T05:08:02.347586Z","shell.execute_reply":"2024-08-25T05:08:02.832910Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\ndef combine_masked_regions(masked_image_path, inverse_masked_image_path, save_path):\n    \"\"\"\n    Combine the original mask areas from the masked image with the inverse-masked image.\n\n    Parameters:\n    - masked_image_path: String, path to the masked image.\n    - inverse_masked_image_path: String, path to the inverse-masked image.\n    - save_path: String, path where the combined image will be saved.\n    \"\"\"\n    # Open images\n    masked_image = Image.open(masked_image_path).convert(\"RGBA\")\n    inverse_masked_image = Image.open(inverse_masked_image_path).convert(\"RGBA\")\n\n    # Ensure images are the same size\n    if masked_image.size != inverse_masked_image.size:\n        raise ValueError(\"Masked and inverse masked images must be the same size\")\n\n    # Convert images to numpy arrays\n    masked_array = np.array(masked_image)\n    inverse_masked_array = np.array(inverse_masked_image)\n\n    # Create a mask where the original mask was applied (non-zero areas in any color channel)\n    mask = np.any(masked_array[..., :3] > 10, axis=-1)\n\n    # Replace inverse-masked image values with masked image values where mask is true\n    combined_array = inverse_masked_array.copy()\n    combined_array[mask] = masked_array[mask]\n\n    # Convert back to image\n    combined_image = Image.fromarray(combined_array)\n\n    # Save the combined image\n    combined_image.save(save_path)\n    print(f\"Combined image saved as {save_path}\")\n\n    # Display the combined image\n    plt.imshow(combined_image)\n    plt.axis('off')\n    plt.show()\n\n# Example usage\nmasked_image_path = \"/kaggle/working/sam_2_augmentation/masked_images/masked_image_1.png\"\ninverse_masked_image_path = \"/kaggle/working/sam_2_augmentation/inverse_masked_images/inverse_masked_image_1.png\"\nsave_path = \"/kaggle/working/sam_2_augmentation/combined_images/combined_image_1.png\"\n\n# Ensure the output directory exists\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n# Combine the images\ncombine_masked_regions(masked_image_path, inverse_masked_image_path, save_path)\n","metadata":{"id":"rZKuNoOXS3DK","outputId":"453bf868-8d09-41c6-fc12-3b194307ba8c","execution":{"iopub.status.busy":"2024-08-25T05:08:13.337728Z","iopub.execute_input":"2024-08-25T05:08:13.338528Z","iopub.status.idle":"2024-08-25T05:08:13.800297Z","shell.execute_reply.started":"2024-08-25T05:08:13.338487Z","shell.execute_reply":"2024-08-25T05:08:13.799334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\ndef pixelate_area(image, mask, pixelation_level):\n    \"\"\"\n    Apply pixelation to the masked area of an image.\n\n    Parameters:\n    - image: NumPy array of the image to be pixelated.\n    - mask: Boolean NumPy array indicating the masked area.\n    - pixelation_level: Int, the size of the blocks used for pixelation.\n    \"\"\"\n    # Create a copy of the image to modify\n    pixelated_image = image.copy()\n\n    # Get image dimensions\n    h, w, _ = image.shape\n\n    # Loop through the masked area and apply pixelation\n    for y in range(0, h, pixelation_level):\n        for x in range(0, w, pixelation_level):\n            # Define the block area\n            block = (slice(y, min(y + pixelation_level, h)), slice(x, min(x + pixelation_level, w)))\n\n            # Check if the block is within the masked area\n            if np.any(mask[block]):\n                # Compute the mean color of the block\n                mean_color = image[block].mean(axis=(0, 1)).astype(int)\n\n                # Apply the mean color to the block\n                pixelated_image[block] = mean_color\n\n    return pixelated_image\n\ndef combine_pixelated_mask(masked_image_path, inverse_masked_image_path, save_path, pixelation_level=10):\n    \"\"\"\n    Combine the pixelated masked areas from the masked image with the inverse-masked image.\n\n    Parameters:\n    - masked_image_path: String, path to the masked image.\n    - inverse_masked_image_path: String, path to the inverse-masked image.\n    - save_path: String, path where the combined image will be saved.\n    - pixelation_level: Int, the size of the blocks used for pixelation.\n    \"\"\"\n    # Open images\n    masked_image = Image.open(masked_image_path).convert(\"RGBA\")\n    inverse_masked_image = Image.open(inverse_masked_image_path).convert(\"RGBA\")\n\n    # Ensure images are the same size\n    if masked_image.size != inverse_masked_image.size:\n        raise ValueError(\"Masked and inverse masked images must be the same size\")\n\n    # Convert images to numpy arrays\n    masked_array = np.array(masked_image)\n    inverse_masked_array = np.array(inverse_masked_image)\n\n    # Create a mask where the original mask was applied (non-zero areas in any color channel)\n    mask = np.any(masked_array[..., :3] > 0, axis=-1)\n\n    # Pixelate the masked area\n    pixelated_mask = pixelate_area(masked_array, mask, pixelation_level)\n\n    # Replace inverse-masked image values with pixelated masked image values where mask is true\n    combined_array = inverse_masked_array.copy()\n    combined_array[mask] = pixelated_mask[mask]\n\n    # Convert back to image\n    combined_image = Image.fromarray(combined_array)\n\n    # Save the combined image\n    combined_image.save(save_path)\n    print(f\"Combined image saved as {save_path}\")\n\n    # Display the combined image\n    plt.imshow(combined_image)\n    plt.axis('off')\n    plt.show()\n\n# Example usage\nmasked_image_path = \"/kaggle/working/sam_2_augmentation/masked_images/masked_image_1.png\"\ninverse_masked_image_path = \"/kaggle/working/sam_2_augmentation/inverse_masked_images/inverse_masked_image_1.png\"\nsave_path = \"/kaggle/working/sam_2_augmentation/combined_images/pixelated_combined_image_1.png\"\n\n# Ensure the output directory exists\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n# Combine the images with pixelated mask\ncombine_pixelated_mask(masked_image_path, inverse_masked_image_path, save_path, pixelation_level=10)\n","metadata":{"id":"lifWU3pJS3AR","outputId":"d917f6a6-8a51-4930-f4bd-54dc7147da4f","execution":{"iopub.status.busy":"2024-08-25T05:08:13.806049Z","iopub.execute_input":"2024-08-25T05:08:13.806704Z","iopub.status.idle":"2024-08-25T05:08:14.312307Z","shell.execute_reply.started":"2024-08-25T05:08:13.806666Z","shell.execute_reply":"2024-08-25T05:08:14.308620Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nimport matplotlib.colors as mcolors\n\ndef change_hue(image, mask, hue_shift):\n    \"\"\"\n    Change the hue of the masked area in an image.\n\n    Parameters:\n    - image: NumPy array of the image to be modified (in RGB).\n    - mask: Boolean NumPy array indicating the masked area.\n    - hue_shift: Float, amount to shift the hue (0 to 1 for a complete cycle).\n    \"\"\"\n    # Convert the image to float in the range [0, 1]\n    float_image = image.astype('float32') / 255.0\n\n    # Convert to HSV\n    hsv_image = mcolors.rgb_to_hsv(float_image)\n\n    # Change the hue in the masked area\n    hsv_image[..., 0][mask] = (hsv_image[..., 0][mask] + hue_shift) % 1.0\n\n    # Convert back to RGB\n    modified_float_image = mcolors.hsv_to_rgb(hsv_image)\n\n    # Scale back to [0, 255]\n    modified_image = (modified_float_image * 255).astype('uint8')\n\n    return modified_image\n\ndef combine_hue_modified_mask(masked_image_path, inverse_masked_image_path, save_path, hue_shift=0.1):\n    \"\"\"\n    Combine the hue-modified masked areas from the masked image with the inverse-masked image.\n\n    Parameters:\n    - masked_image_path: String, path to the masked image.\n    - inverse_masked_image_path: String, path to the inverse-masked image.\n    - save_path: String, path where the combined image will be saved.\n    - hue_shift: Float, amount to shift the hue (0 to 1 for a complete cycle).\n    \"\"\"\n    # Open images\n    masked_image = Image.open(masked_image_path).convert(\"RGBA\")\n    inverse_masked_image = Image.open(inverse_masked_image_path).convert(\"RGBA\")\n\n    # Ensure images are the same size\n    if masked_image.size != inverse_masked_image.size:\n        raise ValueError(\"Masked and inverse masked images must be the same size\")\n\n    # Convert images to numpy arrays\n    masked_array = np.array(masked_image)\n    inverse_masked_array = np.array(inverse_masked_image)\n\n    # Create a mask where the original mask was applied (non-zero areas in any color channel)\n    mask = np.any(masked_array[..., :3] > 0, axis=-1)\n\n    # Change the hue of the masked area\n    hue_modified_mask = change_hue(masked_array[..., :3], mask, hue_shift)\n\n    # Replace inverse-masked image values with hue-modified masked image values where mask is true\n    combined_array = inverse_masked_array.copy()\n    combined_array[mask] = np.dstack((hue_modified_mask, masked_array[..., 3]))[mask]  # Preserve alpha channel\n\n    # Convert back to image\n    combined_image = Image.fromarray(combined_array)\n\n    # Save the combined image\n    combined_image.save(save_path)\n    print(f\"Combined image saved as {save_path}\")\n\n    # Display the combined image\n    plt.imshow(combined_image)\n    plt.axis('off')\n    plt.show()\n\n# Example usage\nmasked_image_path = \"/kaggle/working/sam_2_augmentation/masked_images/masked_image_1.png\"\ninverse_masked_image_path = \"/kaggle/working/sam_2_augmentation/inverse_masked_images/inverse_masked_image_1.png\"\nsave_path = \"/kaggle/working/sam_2_augmentation/combined_images/hue_modified_combined_image_1.png\"\n\n# Ensure the output directory exists\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n# Combine the images with hue-modified mask\ncombine_hue_modified_mask(masked_image_path, inverse_masked_image_path, save_path, hue_shift=0.1)\n","metadata":{"id":"XYaPLE2tTH3h","outputId":"65001c3a-ab07-4b39-bc16-477b02efc3de","execution":{"iopub.status.busy":"2024-08-25T05:08:23.019646Z","iopub.execute_input":"2024-08-25T05:08:23.020794Z","iopub.status.idle":"2024-08-25T05:08:23.644445Z","shell.execute_reply.started":"2024-08-25T05:08:23.020750Z","shell.execute_reply":"2024-08-25T05:08:23.643555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image, ImageFilter\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n\ndef apply_blur_to_masked_area(image, mask, blur_radius=10):\n    \"\"\"\n    Apply a blur effect to the masked area of an image.\n\n    Parameters:\n    - image: PIL Image object of the original image.\n    - mask: Boolean NumPy array indicating the masked area.\n    - blur_radius: Integer, the radius of the Gaussian blur for the blur effect.\n    \"\"\"\n    # Convert image to numpy array\n    image_array = np.array(image)\n\n    # Create a mask image\n    mask_image = Image.fromarray((mask * 255).astype('uint8'), mode='L')\n\n    # Apply a Gaussian blur to the mask image\n    blurred_mask_image = mask_image.filter(ImageFilter.GaussianBlur(radius=blur_radius))\n\n    # Convert the blurred mask to RGB\n    blurred_mask_image = blurred_mask_image.convert('RGB')\n    blurred_mask_array = np.array(blurred_mask_image)\n\n    # Create an image with the same dimensions as the original image\n    blurred_area = np.zeros_like(image_array[..., :3])\n    blurred_area[mask] = blurred_mask_array[mask]\n\n    # Combine the blurred area with the original image\n    combined_array = np.where(blurred_area > 0, blurred_area, image_array[..., :3])\n    combined_image = Image.fromarray(np.uint8(combined_array))\n\n    # Preserve the alpha channel from the original image\n    alpha_channel = image_array[..., 3]\n    combined_image = Image.fromarray(np.dstack((combined_array, alpha_channel)))\n\n    return combined_image\n\ndef combine_and_apply_blur(masked_image_path, inverse_masked_image_path, save_path, blur_radius):\n    \"\"\"\n    Apply a blur effect to the masked image and save the result.\n\n    Parameters:\n    - masked_image_path: String, path to the masked image (used to extract the mask).\n    - inverse_masked_image_path: String, path to the inverse-masked image.\n    - save_path: String, path where the final image will be saved.\n    - blur_radius: Integer, the radius of the Gaussian blur for the blur effect.\n    \"\"\"\n    # Open inverse-masked image\n    inverse_masked_image = Image.open(inverse_masked_image_path).convert(\"RGBA\")\n\n    # Extract the mask from the masked image\n    masked_image = Image.open(masked_image_path).convert(\"L\")\n    mask = np.array(masked_image) > 0\n\n    # Apply blur effect to the masked area\n    blurred_image = apply_blur_to_masked_area(inverse_masked_image, mask, blur_radius)\n\n    # Save the final image\n    blurred_image.save(save_path)\n    print(f\"Final image with blur effect saved as {save_path}\")\n\n    # Display the final image\n    plt.imshow(blurred_image)\n    plt.axis('off')\n    plt.show()\n\n# Example usage\nif __name__ == \"__main__\":\n    blur_radius = 10  # Define the blur radius\n\n    masked_image_path = \"/kaggle/working/sam_2_augmentation/masked_images/masked_image_1.png\"  # Path to the masked image\n    inverse_masked_image_path = \"/kaggle/working/sam_2_augmentation/inverse_masked_images/inverse_masked_image_1.png\"\n    save_path = \"/kaggle/working/sam_2_augmentation/combined_images/blur_combined_image_1.png\"\n\n    # Ensure the output directory exists\n    os.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n    # Apply blur effect to masked area\n    combine_and_apply_blur(masked_image_path, inverse_masked_image_path, save_path, blur_radius)\n","metadata":{"id":"ezYxcRm8TH0y","outputId":"33643977-ac89-458c-a342-75289c33ea4c","execution":{"iopub.status.busy":"2024-08-25T05:08:27.412764Z","iopub.execute_input":"2024-08-25T05:08:27.413553Z","iopub.status.idle":"2024-08-25T05:08:27.931536Z","shell.execute_reply.started":"2024-08-25T05:08:27.413512Z","shell.execute_reply":"2024-08-25T05:08:27.930585Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"a4Slrc1NTHxX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"jAuiBvSATHtu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# image generation","metadata":{"id":"VVOOHCNfTRQq"}},{"cell_type":"code","source":"!pip install stability-sdk","metadata":{"id":"P0I8giUCTHq7","outputId":"4d40938d-3d94-42fe-a7da-9610a3c93b2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport io\nimport warnings\nfrom PIL import Image\nfrom stability_sdk import client\nimport stability_sdk.interfaces.gooseai.generation.generation_pb2 as generation\n\n# Our Host URL should not be prepended with \"https\" nor should it have a trailing slash.\nos.environ['STABILITY_HOST'] = 'grpc.stability.ai:443'\n\n# Sign up for an account at the following link to get an API Key.\n# https://platform.stability.ai/\n\n# Click on the following link once you have created an account to be taken to your API Key.\n# https://platform.stability.ai/account/keys\n\n# Paste your API Key below.\n\nos.environ['STABILITY_KEY'] = 'sk-XIDL92cKTbCsA1BS2MSxJVUZrmogZbA37YWWdq83G5cAgg6z'","metadata":{"id":"Hqr1_cjKTHoO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up our connection to the API.\nstability_api = client.StabilityInference(\n    key=os.environ['STABILITY_KEY'], # API Key reference.\n    verbose=True, # Print debug messages.\n    engine=\"stable-diffusion-xl-1024-v1-0\", # Set the engine to use for generation.\n    # Check out the following link for a list of available engines: https://platform.stability.ai/docs/features/api-parameters#engine\n)","metadata":{"id":"IBBR3fuKTHlj","outputId":"eca5022b-4fb4-4821-a183-0d5d4b9bd73c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nurl = \"/kaggle/working/sam_2_augmentation/masked_images/masked_image_1.png\"\n# Open the image\nimg = Image.open(url)\n\n# Display the image\nplt.imshow(img)\nplt.show()\n\n# # Get the dimensions of the image\nwidth, height = img.size\nprint(f\"Width: {width}, Height: {height}\")\n","metadata":{"id":"pqqTLiaMTHi3","outputId":"0e9633bf-90c2-420e-f272-33918671f41c","execution":{"iopub.status.busy":"2024-08-25T05:08:49.100307Z","iopub.execute_input":"2024-08-25T05:08:49.101108Z","iopub.status.idle":"2024-08-25T05:08:49.380438Z","shell.execute_reply.started":"2024-08-25T05:08:49.101067Z","shell.execute_reply":"2024-08-25T05:08:49.379505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Open your image (this is an example path)\nimage = Image.open(url)\n\n# Convert the image from RGBA to RGB\nrgb_image = image.convert(\"RGB\")\n\n# Save the image as JPEG\nrgb_image.save('/kaggle/working/sam_2_augmentation/output_resized_image.jpg', format=\"JPEG\")\n","metadata":{"id":"ofN40ynwTHWJ","execution":{"iopub.status.busy":"2024-08-25T05:08:54.498062Z","iopub.execute_input":"2024-08-25T05:08:54.498442Z","iopub.status.idle":"2024-08-25T05:08:54.514839Z","shell.execute_reply.started":"2024-08-25T05:08:54.498406Z","shell.execute_reply":"2024-08-25T05:08:54.514045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def resize_image(image_path, output_path, max_size=1024):\n    # Open the image\n    image = Image.open(image_path)\n\n    # Get the current width and height of the image\n    width, height = image.size\n\n    # Calculate the scaling factor\n    if width > height:\n        scaling_factor = max_size / width\n    else:\n        scaling_factor = max_size / height\n\n    # Only resize if the image is larger than the max_size\n    if scaling_factor < 1:\n        # Calculate new dimensions\n        new_width = int(width * scaling_factor)\n        new_height = int(height * scaling_factor)\n\n        # Resize the image\n        image_resized = image.resize((new_width, new_height))\n\n        # Save the resized image\n        image_resized.save(output_path)\n        print(f\"Image resized to {new_width}x{new_height} and saved as {output_path}\")\n    else:\n        # Save the original image without resizing\n        image.save(output_path)\n        print(f\"Image is already within the size limits and saved as {output_path}\")\n\n# Example usage\nresize_image('/kaggle/working/sam_2_augmentation/output_resized_image.jpg', '/kaggle/working/sam_2_augmentation/output_resized_image.jpg')\n","metadata":{"id":"kDAlwnyGTfxt","outputId":"de81326b-08d1-4e23-9d29-b735afc82e19","execution":{"iopub.status.busy":"2024-08-25T05:09:00.375725Z","iopub.execute_input":"2024-08-25T05:09:00.376376Z","iopub.status.idle":"2024-08-25T05:09:00.388141Z","shell.execute_reply.started":"2024-08-25T05:09:00.376336Z","shell.execute_reply":"2024-08-25T05:09:00.387244Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Open the image\nimg = Image.open(\"/kaggle/working/sam_2_augmentation/output_resized_image.jpg\")\n\n# Display the image\nplt.imshow(img)\nplt.show()\n\n# # Get the dimensions of the image\nwidth, height = img.size\nprint(f\"Width: {width}, Height: {height}\")","metadata":{"id":"NdqZLOAZTfuj","outputId":"21bf8162-8715-4362-b210-37b6c385f1d5","execution":{"iopub.status.busy":"2024-08-25T05:09:04.682138Z","iopub.execute_input":"2024-08-25T05:09:04.682791Z","iopub.status.idle":"2024-08-25T05:09:04.974883Z","shell.execute_reply.started":"2024-08-25T05:09:04.682740Z","shell.execute_reply":"2024-08-25T05:09:04.973816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up our initial generation parameters.\nanswers2 = stability_api.generate(\n    prompt=\"body builder,alphonse mucha and simon stalenhag style,\",\n    # guidance_models =\n    init_image=img, # Assign our previously generated img as our Initial Image for transformation.\n    start_schedule=0.6, # Set the strength of our prompt in relation to our initial image.\n    # seed=12343566, # If attempting to transform an image that was previously generated with our API,\n                    # initial images benefit from having their own distinct seed rather than using the seed of the original image generation.\n    steps=250, # Amount of inference steps performed on image generation. Defaults to 30.\n    cfg_scale=10.0, # Influences how strongly your generation is guided to match your prompt.\n                   # Setting this value higher increases the strength in which it tries to match your prompt.\n                   # Defaults to 7.0 if not specified.\n    width=width, # Generation width, defaults to 512 if not included.\n    height=height, # Generation height, defaults to 512 if not included.\n    sampler=generation.SAMPLER_DDIM ,style_preset=\"comic-book\" # Choose which sampler we want to denoise our generation with.\n                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m, k_dpmpp_sde)\n)\n\n# Set up our warning to print to the console if the adult content classifier is tripped.\n# If adult content classifier is not tripped, save generated image.\nfor resp in answers2:\n    for artifact in resp.artifacts:\n        if artifact.finish_reason == generation.FILTER:\n            warnings.warn(\n                \"Your request activated the API's safety filters and could not be processed.\"\n                \"Please modify the prompt and try again.\")\n        if artifact.type == generation.ARTIFACT_IMAGE:\n            global img2\n            img2 = Image.open(io.BytesIO(artifact.binary))\n            img2.save(\"/kaggle/working/sam_2_augmentation/output\"+ \"-img2img.png\") # Save our generated image with its seed number as the filename and the img2img suffix so that we know this is our transformed image.hhhhhhhhhhhhhh\n\n# Open the image\nout_img = Image.open(\"/kaggle/working/sam_2_augmentation/output-img2img.png\")\n\n# Display the image\nplt.imshow(out_img)\nplt.show()","metadata":{"id":"aAKFnTlpTfro","outputId":"df744baa-041d-434f-b0c1-6955f6b9e1e9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"masked_image_path = \"/kaggle/working/sam_2_augmentation/masked_images/masked_image_1.png\"  # Path to the masked imag\ninverse_masked_image_path = \"/kaggle/working/sam_2_augmentation/inverse_masked_images/inverse_masked_image_1.png\"","metadata":{"id":"wC7ACdwkTfoc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Example usage\nimg2img_masked_image_path = \"/kaggle/working/sam_2_augmentation/output-img2img.png\"\ninverse_masked_image_path = \"/kaggle/working/sam_2_augmentation/inverse_masked_images/inverse_masked_image_1.png\"\nimg2img_save_path = \"/kaggle/working/sam_2_augmentation/combined_image_1.png\"\n\n# Ensure the output directory exists\nos.makedirs(os.path.dirname(img2img_save_path), exist_ok=True)\n\n# Combine the images\ncombine_masked_regions(img2img_masked_image_path, inverse_masked_image_path, img2img_save_path)","metadata":{"id":"h4hwWXerTflW","outputId":"8d6f03be-b944-40a7-e546-b162bc189a97"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image\nimport matplotlib.pyplot as plt\nurl = \"/kaggle/working/sam_2_augmentation/inverse_masked_images/inverse_masked_image_1.png\"\n# Open the image\nimg = Image.open(url)\n\n# Display the image\nplt.imshow(img)\nplt.show()\n\n# # Get the dimensions of the image\nwidth, height = img.size\nprint(f\"Width: {width}, Height: {height}\")","metadata":{"id":"nbz2W9qMTfia","outputId":"f9bda738-4520-4b5a-df1f-cd61a16c53d5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Open your image (this is an example path)\nimage = Image.open(url)\n\n# Convert the image from RGBA to RGB\nrgb_image = image.convert(\"RGB\")\n\n# Save the image as JPEG\nrgb_image.save(\"/kaggle/working/sam_2_augmentation/your_inv_image.jpg\", format=\"JPEG\")\n","metadata":{"id":"YgEqi-PATffp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\nresize_image('/kaggle/working/sam_2_augmentation/your_inv_image.jpg', '/kaggle/working/sam_2_augmentation/output_resized_image.jpg')","metadata":{"id":"gVE9zNBVTsUe","outputId":"ec8677f5-6eb9-4a14-c294-1f592d8d96fe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Open the image\nimg = Image.open(\"/kaggle/working/sam_2_augmentation/output_resized_image.jpg\")\n\n# Display the image\nplt.imshow(img)\nplt.show()\n\n# # Get the dimensions of the image\nwidth, height = img.size\nprint(f\"Width: {width}, Height: {height}\")","metadata":{"id":"9F_CBXtVTsQQ","outputId":"076d986e-6a80-44e3-a8ee-346bdd82fc2c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Set up our initial generation parameters.\nanswers2 = stability_api.generate(\n    prompt=\"group of friends in suit,alphonse mucha and simon stalenhag style\",\n    # guidance_models =\n    init_image=img, # Assign our previously generated img as our Initial Image for transformation.\n    start_schedule=0.5, # Set the strength of our prompt in relation to our initial image.\n    # seed=12343566, # If attempting to transform an image that was previously generated with our API,\n                    # initial images benefit from having their own distinct seed rather than using the seed of the original image generation.\n    steps=400, # Amount of inference steps performed on image generation. Defaults to 30.\n    cfg_scale=10.0, # Influences how strongly your generation is guided to match your prompt.\n                   # Setting this value higher increases the strength in which it tries to match your prompt.\n                   # Defaults to 7.0 if not specified.\n    width=width, # Generation width, defaults to 512 if not included.\n    height=height, # Generation height, defaults to 512 if not included.\n    sampler=generation.SAMPLER_DDIM ,style_preset=\"anime\" # Choose which sampler we want to denoise our generation with.\n                                                 # Defaults to k_dpmpp_2m if not specified. Clip Guidance only supports ancestral samplers.\n                                                 # (Available Samplers: ddim, plms, k_euler, k_euler_ancestral, k_heun, k_dpm_2, k_dpm_2_ancestral, k_dpmpp_2s_ancestral, k_lms, k_dpmpp_2m, k_dpmpp_sde)\n)\n\n# Set up our warning to print to the console if the adult content classifier is tripped.\n# If adult content classifier is not tripped, save generated image.\nfor resp in answers2:\n    for artifact in resp.artifacts:\n        if artifact.finish_reason == generation.FILTER:\n            warnings.warn(\n                \"Your request activated the API's safety filters and could not be processed.\"\n                \"Please modify the prompt and try again.\")\n        if artifact.type == generation.ARTIFACT_IMAGE:\n            global img2\n            img2 = Image.open(io.BytesIO(artifact.binary))\n            img2.save(\"/kaggle/working/sam_2_augmentation/output\"+ \"-img2img.png\") # Save our generated image with its seed number as the filename and the img2img suffix so that we know this is our transformed image.hhhhhhhhhhhhhh\n\n# Open the image\nout_img = Image.open(\"/kaggle/working/sam_2_augmentation/output-img2img.png\")\n\n# Display the image\nplt.imshow(out_img)\nplt.show()","metadata":{"id":"lkwffnToTsLI","outputId":"427a00de-d9be-4d0e-876c-6b570f0aee20"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Example usage\nmasked_image_path = \"/kaggle/working/sam_2_augmentation/masked_images/masked_image_1.png\"\ninverse_masked_image_path = \"/kaggle/working/sam_2_augmentation/output-img2img.png\"\nsave_path = \"/kaggle/working/sam_2_augmentation/combined_images/combined_image_1.png\"\n\n# Ensure the output directory exists\nos.makedirs(os.path.dirname(save_path), exist_ok=True)\n\n# Combine the images\ncombine_masked_regions(masked_image_path, inverse_masked_image_path, save_path)\n","metadata":{"id":"8Ph8E27cTsFy","outputId":"33cd24b2-d89e-4bb9-b678-d07b239b4e2d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"huNOX650TsAT"},"execution_count":null,"outputs":[]}]}